{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c9be9586",
   "metadata": {},
   "source": [
    "### Step 1: Convert Slide Images (.ndpi) to JPG Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dd3d5a-1af9-4c64-9f14-7378769c2add",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openslide, os, shutil\n",
    "import pandas as pd\n",
    "\n",
    "os.makedirs('slide_images')\n",
    "stages = ['0','IA', 'IB', 'IIA', 'IIB', 'IIIA', 'IIIB', 'IIIC','IV']\n",
    "for stage in stages:\n",
    "    os.makedirs(f\"slide_images/{stage}\")\n",
    "\n",
    "slides_df = pd.read_csv('./datasets/brca-psj-path/contest-phase-2/slide-manifest-train.csv')\n",
    "outcomes = pd.read_csv('./datasets/brca-psj-path/contest-phase-2/csv-train/outcomes.csv')\n",
    "test_df = slides_df.merge(outcomes[['biopsy_id','patient_ngsci_id','stage']], on=\"biopsy_id\", how=\"left\")\n",
    "paths = test_df.slide_path.to_list()\n",
    "for path in paths:\n",
    "    slide = openslide.open_slide(path)\n",
    "    slide.read_region((0,0), 4, slide.level_dimensions[4]).convert(\"RGB\").save(f\"./slide_images/{path[40:-5]}.jpg\")\n",
    "\n",
    "images = os.listdir('./slide_images')\n",
    "for image in images:\n",
    "    stage = test_df[test_df.slide_id==image[:-4]].reset_index().stage[0]\n",
    "    shutil.move(f\"/home/ngsci/slide_images/{image}\", f\"/home/ngsci/slide_images/{stage}/{image}\")\n",
    "  \n",
    "shutil.move(f\"/home/ngsci/slide_images/IA\", f\"/home/ngsci/slide_images/I\")\n",
    "shutil.move(f\"/home/ngsci/slide_images/IB\", f\"/home/ngsci/slide_images/I\")\n",
    "shutil.move(f\"/home/ngsci/slide_images/IIA\", f\"/home/ngsci/slide_images/II\")\n",
    "shutil.move(f\"/home/ngsci/slide_images/IIB\", f\"/home/ngsci/slide_images/II\")\n",
    "shutil.move(f\"/home/ngsci/slide_images/IIIA\", f\"/home/ngsci/slide_images/III\")\n",
    "shutil.move(f\"/home/ngsci/slide_images/IIIB\", f\"/home/ngsci/slide_images/III\")\n",
    "shutil.move(f\"/home/ngsci/slide_images/IIIC\", f\"/home/ngsci/slide_images/III\")\n",
    "\n",
    "stages.remove['0']\n",
    "stages.remove['IV']\n",
    "for stage in stages:\n",
    "    os.removedirs(f\"./slide_images/{stage}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6202eaa5",
   "metadata": {},
   "source": [
    "### Step 2: Image Preprocessing Training dino-vitb16 Transformers Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5997cd3b-2ff4-4ae4-8515-b1745e93635d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must install datasets from source on nightingale instance:\n",
    "# !cd datasets-main\n",
    "# !pip install -e .\n",
    "# !cd ..\n",
    "# !export TF_ENABLE_ONEDNN_OPTS=0\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification\n",
    "\n",
    "dataset = load_dataset('imagefolder', data_dir='./slide_images')\n",
    "id2label = {id:label for id, label in enumerate(dataset['train'].features['label'].names)}\n",
    "label2id = {label:id for id,label in id2label.items()}\n",
    "\n",
    "# Must download model and upload to nightingale instance:\n",
    "processor = ViTImageProcessor.from_pretrained(\"facebook/dino-vitb16\")\n",
    "model = ViTForImageClassification.from_pretrained(\"facebook/dino-vitb16\", num_labels=5, ignore_mismatched_sizes=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eae4c40-064f-421f-92d7-0bd7fc72eb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import (CenterCrop, \n",
    "                                    Compose, \n",
    "                                    Normalize, \n",
    "                                    RandomHorizontalFlip,\n",
    "                                    RandomResizedCrop, \n",
    "                                    Resize, \n",
    "                                    ToTensor)\n",
    "\n",
    "image_mean, image_std = processor.image_mean, processor.image_std\n",
    "size = processor.size[\"height\"]\n",
    "# size = 224\n",
    "\n",
    "normalize = Normalize(mean=image_mean, std=image_std)\n",
    "_train_transforms = Compose(\n",
    "        [\n",
    "            RandomResizedCrop(size),\n",
    "            RandomHorizontalFlip(),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "_val_transforms = Compose(\n",
    "        [\n",
    "            Resize(size),\n",
    "            CenterCrop(size),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def train_transforms(examples):\n",
    "    examples['pixel_values'] = [_train_transforms(image.convert(\"RGB\")) for image in examples['image']]\n",
    "    return examples\n",
    "\n",
    "dataset['train'].set_transform(train_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c86bf4-84d5-4752-b715-12073b83a5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "args = TrainingArguments(\n",
    "    'vitb16_tuned',\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=10,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=dataset['train'],\n",
    "    data_collator=collate_fn,\n",
    "    tokenizer=processor\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e5749b00",
   "metadata": {},
   "source": [
    "### Step 3: Model Predictions Output and Submission\n",
    "##### Make sure to change the checkpoint path to the best finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dfe0c4-3a6c-450d-81c3-8dac3add2333",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Extracting Holdout Image Set\n",
    "images = os.listdir('./datasets/brca-psj-path/ndpi-holdout')\n",
    "for image in images:\n",
    "    slide = openslide.open_slide(f\"./datasets/brca-psj-path/ndpi-holdout/{image}\")\n",
    "    slide.read_region((0,0), 4, slide.level_dimensions[4]).convert(\"RGB\").save(f\"./slide_images/holdout/{image[:-5]}.jpg\")\n",
    "\n",
    "checkpoint = '' # Insert Checkpoint Path HERE!!!\n",
    "processor = ViTImageProcessor.from_pretrained(checkpoint)\n",
    "model = ViTForImageClassification.from_pretrained(checkpoint)\n",
    "\n",
    "outputs = None\n",
    "logits = None\n",
    "predicted_class_id = None\n",
    "results_df = pd.DataFrame()\n",
    "\n",
    "def infer(slide):\n",
    "    global logits, outputs, predicted_class_id, probs\n",
    "    image = Image.open(slide)\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predicted_class_id = logits.argmax(-1).item()\n",
    "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    print(\"Predicted class:\", model.config.id2label[predicted_class_id])\n",
    "    # print(f\"Predicted class: {predicted_class_id}\")\n",
    "    \n",
    "imageList = os.listdir('slide_images/holdout')       \n",
    "for image in imageList:    \n",
    "    infer(f\"./holdout/{image}\")\n",
    "    results_df.loc[imageList.index(image),0] = str(image[:-4]) # biopsy id\n",
    "    results_df.loc[imageList.index(image),1] = float(probs[0][0].item()) # prob 0\n",
    "    results_df.loc[imageList.index(image),2] = float(probs[0][1].item()) # prob I\n",
    "    results_df.loc[imageList.index(image),3] = float(probs[0][2].item()) # prob II\n",
    "    results_df.loc[imageList.index(image),4] = float(probs[0][3].item()) # prob III\n",
    "    results_df.loc[imageList.index(image),5] = float(probs[0][4].item()) # prob IV\n",
    "    results_df.loc[imageList.index(image),6] = int(predicted_class_id) # predicted class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf5b799-790a-44f3-a2af-4d572f6ae5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final submission CSV (Averaging Probabilities)\n",
    "df = results_df.rename(columns={'0': 'slide_id'})\n",
    "df2 = pd.read_csv('datasets/brca-psj-path/contest-phase-2/slide-manifest-holdout.csv')\n",
    "final_df = df.merge(df2, on='slide_id', how='left')\n",
    "biopsies = final_df.drop_duplicates('biopsy_id')['biopsy_id'].to_list()\n",
    "submit_df = pd.DataFrame(columns=['0','1','2','3','4','5','6'])\n",
    "\n",
    "for biopsy in biopsies:\n",
    "    row = (len(submit_df)+1)\n",
    "    selection = final_df[final_df.biopsy_id==biopsy][['1','2','3','4','5']].mean()\n",
    "    submit_df.loc[row,'0'] = str(biopsy)\n",
    "    submit_df.loc[row,'1'] = float(selection[0])\n",
    "    submit_df.loc[row,'2'] = float(selection[1])\n",
    "    submit_df.loc[row,'3'] = float(selection[2])\n",
    "    submit_df.loc[row,'4'] = float(selection[3])\n",
    "    submit_df.loc[row,'5'] = float(selection[4])\n",
    "    submit_df.loc[row,'6'] = int(list(selection).index(selection.max()))\n",
    "\n",
    "submit_df.to_csv('./project/submit_df.csv', index=False, header=False)\n",
    "\n",
    "import ngsci\n",
    "ngsci.submit_contest_entry(\"./project/submit_df\", description=\"dino-vitb16 trained on level 4 slide images\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
