{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f42708e-daee-47c8-bc76-490d69c837bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import openslide\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "144b9261-8548-4fb3-9baa-b57b9909917c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = './datasets/brca-psj-path/ndpi'\n",
    "slide = openslide.open_slide(f\"{data_dir}/fffe7439-76cd-4335-9f54-d6a1992708f8.ndpi\")\n",
    "#display(slide.get_thumbnail((256,256)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e43259e1-fa2d-441b-8f4d-0ed89cf5b153",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#display(slide.read_region((0,0), 5, slide.level_dimensions[5]))\n",
    "slide.read_region((0,0), 5, slide.level_dimensions[5]).convert(\"RGB\").save('slide1.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dd3d5a-1af9-4c64-9f14-7378769c2add",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openslide, os, shutil\n",
    "import pandas as pd\n",
    "\n",
    "slides_df = pd.read_csv('./datasets/brca-psj-path/contest-phase-2/slide-manifest-train.csv')\n",
    "outcomes = pd.read_csv('./datasets/brca-psj-path/contest-phase-2/csv-train/outcomes.csv')\n",
    "test_df = slides_df.merge(outcomes[['biopsy_id','patient_ngsci_id','stage']], on=\"biopsy_id\", how=\"left\")\n",
    "paths = test_df.slide_path.to_list()\n",
    "\n",
    "for path in paths[8000:]:\n",
    "    slide = openslide.open_slide(path)\n",
    "    slide.read_region((0,0), 4, slide.level_dimensions[4]).convert(\"RGB\").save(f\"./slide_images/{path[40:-5]}.jpg\")\n",
    "\n",
    "images = os.listdir('./slide_images')\n",
    "\n",
    "for image in images:\n",
    "    stage = test_df[test_df.slide_id==image[:-4]].reset_index().stage[0]\n",
    "    shutil.move(f\"/home/ngsci/slide_images/{image}\", f\"/home/ngsci/slide_images/{stage}/{image}\")\n",
    "\n",
    "images = os.listdir('./datasets/brca-psj-path/contest-phase-2/clam-preprocessing-train/stitches')   \n",
    "    \n",
    "# for image in images:\n",
    "#     stage = test_df[test_df.slide_id==image[:-4]].reset_index().stage[0]\n",
    "#     shutil.copy(f\"/home/ngsci/datasets/brca-psj-path/contest-phase-2/clam-preprocessing-train/stitches/{image}\", f\"/home/ngsci/stitches_images/{stage}/{image}\")\n",
    "\n",
    "# mv -v ~/stitches_images/IA/* ~/stitches_images/I\n",
    "# mv -v ~/stitches_images/IB/* ~/stitches_images/I\n",
    "# mv -v ~/stitches_images/IIA/* ~/stitches_images/II\n",
    "# mv -v ~/stitches_images/IIB/* ~/stitches_images/II\n",
    "# mv -v ~/stitches_images/IIIA/* ~/stitches_images/III\n",
    "# mv -v ~/stitches_images/IIIB/* ~/stitches_images/III\n",
    "# mv -v ~/stitches_images/IIIC/* ~/stitches_images/III\n",
    "    \n",
    "    \n",
    "# Holdout\n",
    "images = os.listdir('./datasets/brca-psj-path/ndpi-holdout')\n",
    "\n",
    "for image in images[12000:]:\n",
    "    slide = openslide.open_slide(f\"./datasets/brca-psj-path/ndpi-holdout/{image}\")\n",
    "    slide.read_region((0,0), 4, slide.level_dimensions[4]).convert(\"RGB\").save(f\"./slide_images/holdout/{image[:-5]}.jpg\")\n",
    "    \n",
    "# Spiltting datasets (done for each class)\n",
    "class_IV = os.listdir('./stitch_images/train/IV')\n",
    "for image in range(int(0.15*len(class_IV))):\n",
    "    shutil.move(f\"/home/ngsci/stitch_images/train/IV/{class_IV[image]}\", f\"/home/ngsci/stitch_images/val/IV/{class_IV[image]}\")\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a2d20b-1d2f-4cf9-9b4f-9f1fad8e514c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_values[0][i][z][0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5997cd3b-2ff4-4ae4-8515-b1745e93635d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#must install datasets from source:\n",
    "# cd datasets-main\n",
    "# pip install -e .\n",
    "# cd ..\n",
    "# export TF_ENABLE_ONEDNN_OPTS=0\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset('imagefolder', data_dir='./stitch_images')\n",
    "\n",
    "dataset['train'].features\n",
    "id2label = {id:label for id, label in enumerate(dataset['train'].features['label'].names)}\n",
    "label2id = {label:id for id,label in id2label.items()}\n",
    "id2label\n",
    "\n",
    "# from transformers import ViTImageProcessor, ViTForImageClassification\n",
    "\n",
    "# from transformers import ConvNextImageProcessor, ConvNextForImageClassification\n",
    "\n",
    "# from transformers import AutoImageProcessor, ResNetForImageClassification\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "processor = CLIPProcessor.from_pretrained(\"clip-vit-large-patch14\")\n",
    "model = CLIPModel.from_pretrained(\"clip-vit-large-patch14\", num_labels=5, ignore_mismatched_sizes=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eae4c40-064f-421f-92d7-0bd7fc72eb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import (CenterCrop, \n",
    "                                    Compose, \n",
    "                                    Normalize, \n",
    "                                    RandomHorizontalFlip,\n",
    "                                    RandomResizedCrop, \n",
    "                                    Resize, \n",
    "                                    ToTensor)\n",
    "\n",
    "image_mean, image_std = processor.image_mean, processor.image_std\n",
    "# size = processor.size[\"height\"]\n",
    "size = 224\n",
    "\n",
    "normalize = Normalize(mean=image_mean, std=image_std)\n",
    "_train_transforms = Compose(\n",
    "        [\n",
    "            RandomResizedCrop(size),\n",
    "            RandomHorizontalFlip(),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "_val_transforms = Compose(\n",
    "        [\n",
    "            Resize(size),\n",
    "            CenterCrop(size),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def train_transforms(examples):\n",
    "    examples['pixel_values'] = [_train_transforms(image.convert(\"RGB\")) for image in examples['image']]\n",
    "    return examples\n",
    "\n",
    "def val_transforms(examples):\n",
    "    examples['pixel_values'] = [_val_transforms(image.convert(\"RGB\")) for image in examples['image']]\n",
    "    return examples\n",
    "\n",
    "dataset['train'].set_transform(train_transforms)\n",
    "dataset['validation'].set_transform(val_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c86bf4-84d5-4752-b715-12073b83a5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return dict(accuracy=accuracy_score(predictions, labels))\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# args = TrainingArguments(\n",
    "#     'vitb16_tuned',\n",
    "#     save_strategy=\"epoch\",\n",
    "#     learning_rate=2e-5,\n",
    "#     per_device_train_batch_size=10,\n",
    "#     per_device_eval_batch_size=4,\n",
    "#     num_train_epochs=3,\n",
    "#     weight_decay=0.01,\n",
    "#     remove_unused_columns=False,\n",
    "# )\n",
    "\n",
    "metric_name = \"accuracy\"\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"clip_vit_tuned\",\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=24,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    logging_dir='logs',\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['validation'],\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dfe0c4-3a6c-450d-81c3-8dac3add2333",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inference\n",
    "# !export TF_ENABLE_ONEDNN_OPTS=0\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os, torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset('imagefolder', data_dir='./stitch_images')\n",
    "\n",
    "dataset['train'].features\n",
    "id2label = {id:label for id, label in enumerate(dataset['train'].features['label'].names)}\n",
    "label2id = {label:id for id,label in id2label.items()}\n",
    "id2label\n",
    "\n",
    "processor = ViTImageProcessor.from_pretrained('vitb16_tuned/checkpoint-970')\n",
    "model = ViTForImageClassification.from_pretrained('vitb16_tuned/checkpoint-970')\n",
    "\n",
    "# from transformers import ConvNextImageProcessor, ConvNextForImageClassification\n",
    "\n",
    "# processor = ConvNextImageProcessor.from_pretrained(\"convnext_tuned_masks/checkpoint-1130\")\n",
    "# model = ConvNextForImageClassification.from_pretrained(\"convnext_tuned_masks/checkpoint-1130\")\n",
    "\n",
    "model.config.id2label = id2label\n",
    "model.config.label2id = label2id\n",
    "\n",
    "outputs = None\n",
    "logits = None\n",
    "predicted_class_id = None\n",
    "results_df = pd.DataFrame()\n",
    "\n",
    "def infer(slide):\n",
    "    global logits, outputs, predicted_class_id, probs\n",
    "    image = Image.open(slide)\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predicted_class_id = logits.argmax(-1).item()\n",
    "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    print(\"Predicted class:\", model.config.id2label[predicted_class_id])\n",
    "    # print(f\"Predicted class: {predicted_class_id}\")\n",
    "    \n",
    "\n",
    "imageList = os.listdir('./datasets/brca-psj-path/contest-phase-2/clam-preprocessing-holdout/stitches')\n",
    "       \n",
    "for image in imageList[12000:]:    \n",
    "    infer(f\"./holdout/{image}\")\n",
    "    results_df.loc[imageList.index(image),0] = str(image[:-4]) # biopsy id\n",
    "    results_df.loc[imageList.index(image),1] = float(probs[0][0].item()) # prob 0\n",
    "    results_df.loc[imageList.index(image),2] = float(probs[0][1].item()) # prob I\n",
    "    results_df.loc[imageList.index(image),3] = float(probs[0][2].item()) # prob II\n",
    "    results_df.loc[imageList.index(image),4] = float(probs[0][3].item()) # prob III\n",
    "    results_df.loc[imageList.index(image),5] = float(probs[0][4].item()) # prob IV\n",
    "    results_df.loc[imageList.index(image),6] = int(predicted_class_id) # predicted class\n",
    "\n",
    "\n",
    "# torch.nn.functional.softmax(logits, dim=-1)[0][0].item()\n",
    "\n",
    "# Sample CSV\n",
    "# 47ba1eb2-0d3b-4752-80d3-6d318001751e,0.18312,0.67326,0.11121,0.03231,8.1357e-05,1\n",
    "# e4235769-c290-4bce-bf3a-9b98c7ef80b5,0.27895,0.25605,0.080242,0.38474,2.0360e-06,3\n",
    "# d9bd5e69-98ce-4736-a108-fd64234ffb05,0.0064058,0.16384,0.33646,0.32201,0.17126,2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf5b799-790a-44f3-a2af-4d572f6ae5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final submission csv\n",
    "df = results_df.rename(columns={'0': 'slide_id'})\n",
    "df2 = pd.read_csv('datasets/brca-psj-path/contest-phase-2/slide-manifest-holdout.csv')\n",
    "final_df = df.merge(df2, on='slide_id', how='left')\n",
    "biopsies = final_df.drop_duplicates('biopsy_id')['biopsy_id'].to_list()\n",
    "submit_df = pd.DataFrame(columns=['0','1','2','3','4','5','6'])\n",
    "\n",
    "for biopsy in biopsies:\n",
    "    row = (len(submit_df)+1)\n",
    "    selection = final_df[final_df.biopsy_id==biopsy][['1','2','3','4','5']].mean()\n",
    "    submit_df.loc[row,'0'] = str(biopsy)\n",
    "    submit_df.loc[row,'1'] = float(selection[0])\n",
    "    submit_df.loc[row,'2'] = float(selection[1])\n",
    "    submit_df.loc[row,'3'] = float(selection[2])\n",
    "    submit_df.loc[row,'4'] = float(selection[3])\n",
    "    submit_df.loc[row,'5'] = float(selection[4])\n",
    "    submit_df.loc[row,'6'] = int(list(selection).index(selection.max()))\n",
    "\n",
    "# Change probability classification of class 0 breaat cancer (>15%)\n",
    "submit_df.loc[submit_df['1']>0.15, '6'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be1455d-2994-4191-a7f8-763065016904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histoseg\n",
    "\n",
    "import numpy as np\n",
    "import os, cv2\n",
    "from PIL import Image\n",
    "\n",
    "height = 1000\n",
    "width = 1000\n",
    "channels = 3\n",
    "dim = (width, height)\n",
    "imageList = os.listdir('./slide_images/0')\n",
    "x = np.ndarray(shape=(len(imageList[:10]),height,width,channels), dtype='uint8')\n",
    "\n",
    "\n",
    "for i in range(len(imageList[:10])):\n",
    "      # open image to numpy array\n",
    "      img = Image.open(f\"./slide_images/0/{imageList[i]}\")\n",
    "      img = img.resize((height,width))       \n",
    "      # insert into placeholder array\n",
    "      x[i] = np.array(img)\n",
    "\n",
    "# for i in range(len(imageList[:10])):\n",
    "#       # open image to numpy array\n",
    "#       img = cv2.imread(f\"./slide_images/0/{imageList[i]}\")\n",
    "#       # resize\n",
    "#       img = cv2.resize(img, dim, interpolation = cv2.INTER_AREA) \n",
    "#       # insert into placeholder array\n",
    "#       x[i] = img\n",
    "\n",
    "np.save('dataset.npy', x)\n",
    "    \n",
    "# np.savez('dataset.npz', x=x)\n",
    "\n",
    "# img2 = Image.fromarray(img2, 'RGB')\n",
    "# img2.save('my.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016ae473-cc9f-40a4-9b78-1bfaae259895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !export TF_ENABLE_ONEDNN_OPTS=0\n",
    "python HistoSeg_Test.py --images 'dataset.npz' --weights 'HistoSeg_MoNuSeg_.h5' --width 1000 --height 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cfbb52-168c-4e9d-b73d-ffea3d7ee929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EfficientViT \n",
    "python -m torch.distributed.launch --nproc_per_node=4 --master_port 12345 --use_env main.py --model EfficientViT_M5 --data-path /home/ngsci/stitch_images --dist-eval\n",
    "/home/ngsci/slide_images\n",
    "\n",
    "# InceptionResnetv2\n",
    "python image_train.py \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
